{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# data fetching...............\n",
    "\n",
    "offset_val = 0\n",
    "\n",
    "# Define the base URL without filter parameters\n",
    "base_url = \"https://data.exactspace.co/exactapi/units/60ae9143e284d016d3559dfb/activities\"\n",
    "\n",
    "all_objects = []  # List to store all fetched objects\n",
    "\n",
    "while len(all_objects) < 900:  # Continue fetching until desired number is reached\n",
    "    \n",
    "    # Construct the URL with the updated offset value\n",
    "    url = f\"{base_url}?filter=%7B%20%20%20%20%20%22where%22%3A%20%7B%20%20%20%20%20%20%20%20%20%22type%22%3A%20%22task%22%20%20%7D%2C%20%20%20%20%20%22order%22%3A%20%22createdOn%20DESC%22%2C%20%20%20%20%20%22limit%22%3A%2010%2C%20%22offset%22%3A{offset_val}%20%7D&access_token=Ziv35SH8DYvWbh0ZL8HMhOCESfQTwajS7j8iHfb1A3pEoSO6P8NraCLmCD1g7iAH\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        objects_fetched = len(data)  # Number of objects fetched in this request\n",
    "        all_objects.extend(data)      # Add fetched objects to the list\n",
    "        offset_val += 10\n",
    "        print(\"offset success:\", offset_val)\n",
    "    else:\n",
    "        print(\"Failed to fetch data. Status code:\", response.status_code)\n",
    "        break\n",
    "\n",
    "print(\"Total objects fetched:\", len(all_objects))\n",
    "\n",
    "\n",
    "# recommendation sheet........\n",
    "\n",
    "tagList = [\"GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric\"]\n",
    "\n",
    "def getValues4(tagList, startDate, endDate):\n",
    "    url =  \"https://data.exactspace.co/kairosapi/api/v1/datapoints/query\"\n",
    "    d = {\n",
    "        \"metrics\": [\n",
    "            {\n",
    "                \"tags\": {},\n",
    "                \"name\": \"\",\n",
    "            }\n",
    "        ],\n",
    "        \"plugins\": [],\n",
    "        \"cache_time\": 0,\n",
    "        \"start_absolute\": startDate,\n",
    "        \"end_absolute\": endDate\n",
    "    }\n",
    "    dfs = []\n",
    "    for tag in tagList:\n",
    "        d['metrics'][0]['name'] = tag\n",
    "        res = requests.post(url=url, json=d)\n",
    "        values = json.loads(res.content)\n",
    "        df = pd.DataFrame(values[\"queries\"][0][\"results\"][0]['values'], columns=['time', values[\"queries\"][0][\"results\"][0]['name']])\n",
    "        df['time'] = pd.to_datetime(df['time'], unit='ms') + pd.Timedelta(hours=5.5)\n",
    "        df.sort_values(by='time', inplace=True)\n",
    "        df = df.drop_duplicates(subset=[\"time\", tag])\n",
    "        if df.shape[0] < 10:\n",
    "            pass\n",
    "        else:\n",
    "            dfs.append(df)\n",
    "    df = dfs[0]\n",
    "    for df_ in dfs[1:]:\n",
    "        df = pd.merge(df, df_, on='time')\n",
    "    return df\n",
    "\n",
    "# Function to convert ISO 8601 timestamp to epoch timestamp\n",
    "def iso_to_epoch(iso_timestamp):\n",
    "    return int(datetime.fromisoformat(iso_timestamp[:-1]).timestamp())\n",
    "\n",
    "# Extract recommendation data from JSON and create DataFrame\n",
    "def extract_recommendations(all_objects):\n",
    "    recommendations = []\n",
    "    for obj in all_objects:\n",
    "        title = obj.get(\"content\", [{}])[0].get(\"value\", \"\")\n",
    "        if \"GAP: GSD Recommendations\" in title:\n",
    "            recommendation_text = obj.get(\"content\", [{}])[1].get(\"value\", \"\")  # Assuming the recommendation text is in the second item of the content list\n",
    "            update_history = obj.get(\"updateHistory\", [])\n",
    "            update_actions = [entry.get(\"action\", \"\") for entry in update_history]  # Extract actions from updateHistory\n",
    "            actions_str = \"\\n\".join(update_actions)  # Concatenate actions into a single string separated by newline\n",
    "            created_time = obj.get(\"createdOn\")\n",
    "            due_time = obj.get(\"dueDate\")\n",
    "            startDate = iso_to_epoch(created_time) * 1000\n",
    "            endDate = iso_to_epoch(due_time) * 1000\n",
    "            # Fetch data for the specified time range\n",
    "            data = getValues4(tagList, startDate, endDate)\n",
    "            data = data.tail(1)\n",
    "            data = data.round(4)\n",
    "            data = data.drop(columns=['time'])\n",
    "            density = data.to_dict('records')\n",
    "            result = density[0].get(\"GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric\")\n",
    "            if result > 1.65:\n",
    "                density_achieved = \"Yes\"\n",
    "            else:\n",
    "                density_achieved = \"No\"\n",
    "            recommendations.append({\"ID\": obj.get(\"id\", \"\"), \"Recommendation\": recommendation_text, \"Density Achieved\": density_achieved, \"Update Actions\": actions_str})\n",
    "    return recommendations\n",
    "\n",
    "# Extract data from JSON and write to Excel\n",
    "def extract_and_write_to_excel(all_objects):\n",
    "    recommendations = extract_recommendations(all_objects)\n",
    "    if not recommendations:\n",
    "        print(\"No recommendations found.\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(recommendations)\n",
    "    \n",
    "    # Define writer\n",
    "    writer = pd.ExcelWriter(\"recommendations.xlsx\", engine=\"xlsxwriter\")\n",
    "    df.to_excel(writer, index=False, sheet_name=\"Recommendations\")\n",
    "    worksheet = writer.sheets[\"Recommendations\"]\n",
    "    \n",
    "    # Set column width\n",
    "    worksheet.set_column('A:Z', 15)  # Set column width for ID\n",
    "    \n",
    "    # Increase row height for cells with long text\n",
    "    for i, text in enumerate(df[\"Recommendation\"], start=1):\n",
    "        num_lines = text.count(\"\\n\") + 1  # Count the number of lines in the text\n",
    "        if num_lines > 1:\n",
    "            worksheet.set_row(i, 20 * num_lines)  # Set row height to accommodate the text\n",
    "        \n",
    "    writer.save()\n",
    "    print(\"Excel file saved successfully.\")\n",
    "\n",
    "# Assuming `all_objects` is defined somewhere\n",
    "extract_and_write_to_excel(all_objects)\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = 'recommendations.xlsx'\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Define the phrase to remove\n",
    "phrase_to_remove = \"This task is created by Pulse.\"\n",
    "\n",
    "# Remove the phrase from the specified column\n",
    "df['Update Actions'] = df['Update Actions'].str.replace(phrase_to_remove, '')\n",
    "\n",
    "# Save the modified DataFrame back to Excel\n",
    "output_file = 'final_report.xlsx'\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"Data cleaned and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "def getValues(tagList, start ,end):\n",
    "        url = \"https://data.exactspace.co/kairosapi/api/v1/datapoints/query\"\n",
    "        d = {\n",
    "            \"metrics\": [\n",
    "                {\n",
    "                    \"tags\": {},\n",
    "                    \"name\": \"\"\n",
    "                }\n",
    "            ],\n",
    "            \"plugins\": [],\n",
    "            \"cache_time\": 0,\n",
    "            \"cache_time\": 0,\n",
    "            \"start_absolute\": start,\n",
    "            \"end_absolute\": end\n",
    "        }\n",
    "        finalDF = pd.DataFrame()\n",
    "        for tag in tagList:\n",
    "            d['metrics'][0]['name'] = tag\n",
    "            res = requests.post(url=url, json=d)\n",
    "            values = json.loads(res.content)\n",
    "            df = pd.DataFrame(values[\"queries\"][0][\"results\"][0]['values'], columns=['time', values[\"queries\"][0][\"results\"][0]['name']])\n",
    "            finalDF = pd.concat([finalDF, df], axis=1)\n",
    "\n",
    "        finalDF = finalDF.loc[:, ~finalDF.columns.duplicated()]\n",
    "        finalDF.dropna(subset=['time'], inplace=True)\n",
    "        # finalDF['time'] = pd.to_datetime(finalDF['time'], unit='ms').dt.strftime('%d-%m-%y %H:%M:%S')\n",
    "        return finalDF\n",
    "\n",
    "def mld_2_anode_status(start, end):\n",
    "    tags= [\n",
    "            'GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric',\n",
    "            'GAP_GAP03.PLC03.SCHENCK2_FEED_RATE',\n",
    "            'GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'\n",
    "        ]\n",
    "    \n",
    "    temp_df = pd.DataFrame(columns=['time'])\n",
    "    temp_df.loc[0, 'time'] = start  # Insert 42 into 'Good' column at index 0\n",
    "    temp_df.loc[1, 'time'] = end\n",
    "    temp_df['time'] += (5 * 60 + 30) * 60 * 1000\n",
    "    temp_df['date'] = pd.to_datetime(temp_df['time'], unit='ms')\n",
    "\n",
    "    mld_2_df=getValues(tags, start, end)\n",
    "    \n",
    "    mld_2_df = mld_2_df.dropna()\n",
    "    mld_2_df = mld_2_df[(mld_2_df['GAP_GAP03.PLC03.SCHENCK2_FEED_RATE'] >= 4000) & (mld_2_df['GAP_GAP03.PLC03.SCHENCK2_FEED_RATE'] < 6700) \n",
    "        & (mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric'] >= 1.56) & (mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric'] <= 1.69)]\n",
    "    \n",
    "    mld_2_df = mld_2_df[mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'] % 1 == 0]\n",
    "    mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'] = mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'].astype(int)\n",
    "\n",
    "    # Find consecutive duplicate values in the column and mark them for removal\n",
    "    mld_2_df['to_remove'] = mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'] == mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'].shift(1)\n",
    "\n",
    "    mld_2_df = mld_2_df[mld_2_df['to_remove'] != True]\n",
    "    \n",
    "    mld_2_df.reset_index(drop=True, inplace=True)\n",
    "    # print(mld_2_df)\n",
    "    # Assuming df is your DataFrame\n",
    "    # Convert the 'time' column to datetime format\n",
    "    mld_2_df['time'] += (5 * 60 + 30) * 60 * 1000\n",
    "    mld_2_df['date'] = pd.to_datetime(mld_2_df['time'], unit='ms')\n",
    "\n",
    "    # Define conditions for categorizing 'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'\n",
    "    condition_better = mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric'] >= 1.66\n",
    "    condition_good = (mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric'] >= 1.65) & (mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric'] < 1.66)\n",
    "    condition_bad = mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric'] < 1.65\n",
    "\n",
    "    # Create a new column 'category' based on conditions\n",
    "    mld_2_df['category'] = 'Undefined'\n",
    "    mld_2_df.loc[condition_better, 'category'] = 'Better'\n",
    "    mld_2_df.loc[condition_good, 'category'] = 'Good'\n",
    "    mld_2_df.loc[condition_bad, 'category'] = 'Bad'\n",
    "\n",
    "    # Create an empty DataFrame with columns\n",
    "    hourly_counts = pd.DataFrame(columns=['Bad', 'Better', 'Good', 'time'])\n",
    "\n",
    "    # Group by hour and category, then count occurrences\n",
    "    hourly_counts = mld_2_df.groupby([mld_2_df['date'].dt.floor('H'), 'category']).size().unstack(fill_value=0)\n",
    "\n",
    "    complete_hourly_index = pd.date_range(temp_df['date'].min(), temp_df['date'].max(), freq='H')\n",
    "    reference_df = pd.DataFrame(complete_hourly_index, columns=['date'])\n",
    "\n",
    "    # Add columns for 'Good', 'Better', and 'Bad' with zeros\n",
    "    reference_df['Good'] = 0\n",
    "    reference_df['Better'] = 0\n",
    "    reference_df['Bad'] = 0\n",
    "\n",
    "    missing_categories = set(['Bad', 'Better', 'Good']) - set(hourly_counts.columns)\n",
    "    for category in missing_categories:\n",
    "            hourly_counts[category] = 0\n",
    "\n",
    "    hourly_counts = pd.merge(reference_df, hourly_counts, how='left', on=['date'])\n",
    "    hourly_counts = hourly_counts[['date', 'Good_y', 'Better_y', 'Bad_y']]\n",
    "    hourly_counts = hourly_counts.rename(columns={'Good_y': 'Good', 'Better_y': 'Better', 'Bad_y':'Bad'})\n",
    "\n",
    "    hourly_counts = hourly_counts.fillna(0)\n",
    "    return hourly_counts\n",
    "\n",
    "def mld_1_anode_status(start, end):\n",
    "        tags= [\n",
    "                'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric',\n",
    "                'GAP_GAP03.PLC03.SCHENCK2_FEED_RATE',\n",
    "                'GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'\n",
    "            ]\n",
    "\n",
    "        temp_df = pd.DataFrame(columns=['time'])\n",
    "        temp_df.loc[0, 'time'] = start  # Insert 42 into 'Good' column at index 0\n",
    "        temp_df.loc[1, 'time'] = end\n",
    "        temp_df['time'] += (5 * 60 + 30) * 60 * 1000\n",
    "        temp_df['date'] = pd.to_datetime(temp_df['time'], unit='ms')\n",
    "\n",
    "        mld_1_df=getValues(tags, start, end)\n",
    "\n",
    "        mld_1_df = mld_1_df.dropna()\n",
    "        mld_1_df = mld_1_df[(mld_1_df['GAP_GAP03.PLC03.SCHENCK2_FEED_RATE'] >= 4000) & (mld_1_df['GAP_GAP03.PLC03.SCHENCK2_FEED_RATE'] < 6700) \n",
    "            & (mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] >= 1.56) & (mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] <= 1.69)]\n",
    "\n",
    "        mld_1_df = mld_1_df[mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] % 1 == 0]\n",
    "        mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] = mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'].astype(int)\n",
    "\n",
    "        # Find consecutive duplicate values in the column and mark them for removal\n",
    "        mld_1_df['to_remove'] = mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] == mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'].shift(1)\n",
    "\n",
    "        mld_1_df = mld_1_df[mld_1_df['to_remove'] != True]\n",
    "\n",
    "        mld_1_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "        mld_1_df['time'] += (5 * 60 + 30) * 60 * 1000\n",
    "        mld_1_df['date'] = pd.to_datetime(mld_1_df['time'], unit='ms')\n",
    "\n",
    "        # Define conditions for categorizing 'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'\n",
    "        condition_better = mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] >= 1.66\n",
    "        condition_good = (mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] >= 1.65) & (mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] < 1.66)\n",
    "        condition_bad = mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] < 1.65\n",
    "\n",
    "        # Create a new column 'category' based on conditions\n",
    "        mld_1_df['category'] = 'Undefined'\n",
    "        mld_1_df.loc[condition_better, 'category'] = 'Better'\n",
    "        mld_1_df.loc[condition_good, 'category'] = 'Good'\n",
    "        mld_1_df.loc[condition_bad, 'category'] = 'Bad'\n",
    "\n",
    "        # Create an empty DataFrame with columns\n",
    "        hourly_counts = pd.DataFrame(columns=['Bad', 'Better', 'Good', 'time'])\n",
    "\n",
    "        # Group by hour and category, then count occurrences\n",
    "        hourly_counts = mld_1_df.groupby([mld_1_df['date'].dt.floor('H'), 'category']).size().unstack(fill_value=0)\n",
    "\n",
    "        complete_hourly_index = pd.date_range(temp_df['date'].min(), temp_df['date'].max(), freq='H')\n",
    "        reference_df = pd.DataFrame(complete_hourly_index, columns=['date'])\n",
    "\n",
    "        # Add columns for 'Good', 'Better', and 'Bad' with zeros\n",
    "        reference_df['Good'] = 0\n",
    "        reference_df['Better'] = 0\n",
    "        reference_df['Bad'] = 0\n",
    "\n",
    "        missing_categories = set(['Bad', 'Better', 'Good']) - set(hourly_counts.columns)\n",
    "        for category in missing_categories:\n",
    "                hourly_counts[category] = 0\n",
    "\n",
    "        hourly_counts = pd.merge(reference_df, hourly_counts, how='left', on=['date'])\n",
    "        hourly_counts = hourly_counts[['date', 'Good_y', 'Better_y', 'Bad_y']]\n",
    "        hourly_counts = hourly_counts.rename(columns={'Good_y': 'Good', 'Better_y': 'Better', 'Bad_y':'Bad'})\n",
    "\n",
    "        hourly_counts = hourly_counts.fillna(0)\n",
    "        return hourly_counts\n",
    "\n",
    "def kpis_dashboard(start, end):\n",
    "        tags= [\n",
    "        'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric',\n",
    "        'GAP_GAP03.PLC03.SCHENCK2_FEED_RATE',\n",
    "        'GAP_GAP04.PLC04.MLD1_DATA_Anode_Number',\n",
    "        'GAP_GAP04.PLC04.MLD1_DATA_Anode_Weight',\n",
    "        'GAP_GAP04.PLC04.MLD1_DATA_Anode_Height',\n",
    "        'GAP_GAP04.PLC04.MLD1_DATA_Anode_Dry_Density'\n",
    "        ]\n",
    "\n",
    "        temp_df = pd.DataFrame(columns=['time'])\n",
    "        temp_df.loc[0, 'time'] = start  # Insert 42 into 'Good' column at index 0\n",
    "        temp_df.loc[1, 'time'] = end\n",
    "        temp_df['time'] += (5 * 60 + 30) * 60 * 1000\n",
    "        temp_df['date'] = pd.to_datetime(temp_df['time'], unit='ms')\n",
    "\n",
    "        mld_1_df=getValues(tags, start, end)\n",
    "\n",
    "        mld_1_df = mld_1_df.dropna()\n",
    "        mld_1_df = mld_1_df[(mld_1_df['GAP_GAP03.PLC03.SCHENCK2_FEED_RATE'] >= 4000) & (mld_1_df['GAP_GAP03.PLC03.SCHENCK2_FEED_RATE'] < 6700) \n",
    "        & (mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] >= 1.56) & (mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] <= 1.69)]\n",
    "\n",
    "        mld_1_df = mld_1_df[mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] % 1 == 0]\n",
    "        mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] = mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'].astype(int)\n",
    "\n",
    "        # Find consecutive duplicate values in the column and mark them for removal\n",
    "        mld_1_df['to_remove'] = mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] == mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'].shift(1)\n",
    "\n",
    "        mld_1_df = mld_1_df[mld_1_df['to_remove'] != True]\n",
    "\n",
    "        mld_1_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "        mld_1_df['time'] += (5 * 60 + 30) * 60 * 1000\n",
    "        mld_1_df['date'] = pd.to_datetime(mld_1_df['time'], unit='ms')\n",
    "        mld_1_df.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "        columns_of_interest = [\n",
    "            'GAP_GAP04.PLC04.MLD1_DATA_Anode_Weight',\n",
    "            'GAP_GAP04.PLC04.MLD1_DATA_Anode_Dry_Density',\n",
    "            'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric',\n",
    "            'GAP_GAP04.PLC04.MLD1_DATA_Anode_Height'\n",
    "        ]\n",
    "\n",
    "        # Calculate hourly mean for multiple columns\n",
    "        hourly_mean = mld_1_df[columns_of_interest].resample('H').mean()\n",
    "        hourly_std = mld_1_df[columns_of_interest].resample('H').std()\n",
    "        hourly_std = hourly_std.round(3)\n",
    "        hourly_mean = hourly_mean.round(3)\n",
    "\n",
    "        hourly_std.reset_index(inplace=True)\n",
    "        hourly_mean.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "        complete_hourly_index = pd.date_range(temp_df['date'].min(), temp_df['date'].max(), freq='H')\n",
    "        reference_df = pd.DataFrame(complete_hourly_index, columns=['date'])\n",
    "\n",
    "        hourly_mean = pd.merge(reference_df, hourly_mean, how='left', on=['date'])\n",
    "        hourly_std = pd.merge(reference_df, hourly_std, how='left', on=['date'])\n",
    "        global test_df\n",
    "        test_df = hourly_std.copy()\n",
    "\n",
    "        missing_categories1 = set(['GAP_GAP04.PLC04.MLD1_DATA_Anode_Weight',\n",
    "            'GAP_GAP04.PLC04.MLD1_DATA_Anode_Dry_Density',\n",
    "            'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric',\n",
    "            'GAP_GAP04.PLC04.MLD1_DATA_Anode_Height']) - set(hourly_mean.columns)\n",
    "\n",
    "        missing_categories2 = set(['GAP_GAP04.PLC04.MLD1_DATA_Anode_Weight',\n",
    "            'GAP_GAP04.PLC04.MLD1_DATA_Anode_Dry_Density',\n",
    "            'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric',\n",
    "            'GAP_GAP04.PLC04.MLD1_DATA_Anode_Height']) - set(hourly_std.columns)\n",
    "\n",
    "        for category in missing_categories1:\n",
    "                hourly_mean[category] = 0\n",
    "\n",
    "        for category in missing_categories2:\n",
    "                hourly_std[category] = 0\n",
    "\n",
    "        hourly_mean = hourly_mean.fillna(0)\n",
    "        hourly_std = hourly_std.fillna(0)\n",
    "\n",
    "        hourly_mean.set_index('date', inplace=True)\n",
    "        hourly_std.set_index('date', inplace=True)\n",
    "\n",
    "        # Convert the index to epoch format\n",
    "        hourly_mean[\"time\"] = hourly_mean.index.astype(int) // 10**6  \n",
    "        hourly_mean[\"time\"] -= (5 * 60 + 30) * 60 * 1000\n",
    "\n",
    "        hourly_std[\"time\"] = hourly_std.index.astype(int) // 10**6  \n",
    "        hourly_std[\"time\"] -= (5 * 60 + 30) * 60 * 1000\n",
    "\n",
    "        return hourly_mean, hourly_std\n",
    "\n",
    "start = 1701369000000\n",
    "end = 1703961000000\n",
    "\n",
    "hourly_mld2_anode_status = mld_2_anode_status(start, end)\n",
    "mld2_bad = hourly_mld2_anode_status['Bad'].sum()\n",
    "mld2_better = hourly_mld2_anode_status['Better'].sum()\n",
    "mld2_good = hourly_mld2_anode_status['Good'].sum()\n",
    "\n",
    "hourly_mld1_anode_status = mld_1_anode_status(start, end)\n",
    "mld1_bad = hourly_mld1_anode_status['Bad'].sum()\n",
    "mld1_better = hourly_mld1_anode_status['Better'].sum()\n",
    "mld1_good = hourly_mld1_anode_status['Good'].sum()\n",
    "\n",
    "hourly_means_kpis, hourly_std_kpis = kpis_dashboard(start, end)\n",
    "\n",
    "# # Define shorter column names\n",
    "# new_column_names = {\n",
    "#     'GAP_GAP04.PLC04.MLD1_DATA_Anode_Weight': 'Weight',\n",
    "#     'GAP_GAP04.PLC04.MLD1_DATA_Anode_Dry_Density': 'Dry_Density',\n",
    "#     'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric': 'Geometric',\n",
    "#     'GAP_GAP04.PLC04.MLD1_DATA_Anode_Height': 'Height'\n",
    "# }\n",
    "\n",
    "# columns_to_format = ['GAP_GAP04.PLC04.MLD1_DATA_Anode_Weight', 'GAP_GAP04.PLC04.MLD1_DATA_Anode_Dry_Density', 'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric', 'GAP_GAP04.PLC04.MLD1_DATA_Anode_Height']\n",
    "\n",
    "# # Format the selected columns\n",
    "# formatted_series = hourly_std_kpis[columns_to_format].applymap(lambda x: \"{:.3f}\".format(x))\n",
    "\n",
    "# # Rename the columns\n",
    "# formatted_and_renamed_series_kpis = formatted_series.rename(columns=new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vikra\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:259: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n",
      "c:\\Users\\vikra\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:262: FutureWarning: casting datetime64[ns] values to int64 with .astype(...) is deprecated and will raise in a future version. Use .view(...) instead.\n"
     ]
    }
   ],
   "source": [
    "hourly_means_kpis, hourly_std_kpis = kpis_dashboard(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shorter column names\n",
    "new_column_names = {\n",
    "    'GAP_GAP04.PLC04.MLD1_DATA_Anode_Weight': 'Anode Weight',\n",
    "    'GAP_GAP04.PLC04.MLD1_DATA_Anode_Dry_Density': 'Anode Dry Density',\n",
    "    'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric': 'Anode Geometric Density',\n",
    "    'GAP_GAP04.PLC04.MLD1_DATA_Anode_Height': 'Anode Height'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_means_kpis = hourly_means_kpis.rename(columns=new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Anode Weight               1.065279e+03\n",
       "Anode Dry Density          1.439107e+00\n",
       "Anode Geometric Density    1.654734e+00\n",
       "Anode Height               6.804077e+02\n",
       "time                       1.702562e+12\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_means_kpis[hourly_means_kpis['Anode Dry Density']>0].round(4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
