{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValues(tagList, start ,end):\n",
    "        url = \"https://data.exactspace.co/kairosapi/api/v1/datapoints/query\"\n",
    "        d = {\n",
    "            \"metrics\": [\n",
    "                {\n",
    "                    \"tags\": {},\n",
    "                    \"name\": \"\"\n",
    "                }\n",
    "            ],\n",
    "            \"plugins\": [],\n",
    "            \"cache_time\": 0,\n",
    "            \"cache_time\": 0,\n",
    "            \"start_absolute\": start,\n",
    "            \"end_absolute\": end\n",
    "        }\n",
    "        finalDF = pd.DataFrame()\n",
    "        for tag in tagList:\n",
    "            d['metrics'][0]['name'] = tag\n",
    "            res = requests.post(url=url, json=d)\n",
    "            values = json.loads(res.content)\n",
    "            df = pd.DataFrame(values[\"queries\"][0][\"results\"][0]['values'], columns=['time', values[\"queries\"][0][\"results\"][0]['name']])\n",
    "            finalDF = pd.concat([finalDF, df], axis=1)\n",
    "\n",
    "        finalDF = finalDF.loc[:, ~finalDF.columns.duplicated()]\n",
    "        finalDF.dropna(subset=['time'], inplace=True)\n",
    "        # finalDF['time'] = pd.to_datetime(finalDF['time'], unit='ms').dt.strftime('%d-%m-%y %H:%M:%S')\n",
    "        return finalDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mld_2_anode_status(start, end):\n",
    "    tags= [\n",
    "            'GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric',\n",
    "            'GAP_GAP03.PLC03.SCHENCK2_FEED_RATE',\n",
    "            'GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'\n",
    "        ]\n",
    "    \n",
    "    temp_df = pd.DataFrame(columns=['time'])\n",
    "    temp_df.loc[0, 'time'] = start  # Insert 42 into 'Good' column at index 0\n",
    "    temp_df.loc[1, 'time'] = end\n",
    "    temp_df['time'] += (5 * 60 + 30) * 60 * 1000\n",
    "    temp_df['date'] = pd.to_datetime(temp_df['time'], unit='ms')\n",
    "\n",
    "    mld_2_df=getValues(tags, start, end)\n",
    "    \n",
    "    mld_2_df = mld_2_df.dropna()\n",
    "    mld_2_df = mld_2_df[(mld_2_df['GAP_GAP03.PLC03.SCHENCK2_FEED_RATE'] >= 4000) & (mld_2_df['GAP_GAP03.PLC03.SCHENCK2_FEED_RATE'] < 6700) \n",
    "        & (mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric'] >= 1.56) & (mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric'] <= 1.69)]\n",
    "    \n",
    "    mld_2_df = mld_2_df[mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'] % 1 == 0]\n",
    "    mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'] = mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'].astype(int)\n",
    "\n",
    "    # Find consecutive duplicate values in the column and mark them for removal\n",
    "    mld_2_df['to_remove'] = mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'] == mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Number'].shift(1)\n",
    "\n",
    "    mld_2_df = mld_2_df[mld_2_df['to_remove'] != True]\n",
    "    \n",
    "    mld_2_df.reset_index(drop=True, inplace=True)\n",
    "    # print(mld_2_df)\n",
    "    # Assuming df is your DataFrame\n",
    "    # Convert the 'time' column to datetime format\n",
    "    mld_2_df['time'] += (5 * 60 + 30) * 60 * 1000\n",
    "    mld_2_df['date'] = pd.to_datetime(mld_2_df['time'], unit='ms')\n",
    "\n",
    "    # Define conditions for categorizing 'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'\n",
    "    condition_better = mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric'] >= 1.66\n",
    "    condition_good = (mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric'] >= 1.65) & (mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric'] < 1.66)\n",
    "    condition_bad = mld_2_df['GAP_GAP04.PLC04.MLD2_DATA_Anode_Geometric'] < 1.65\n",
    "\n",
    "    # Create a new column 'category' based on conditions\n",
    "    mld_2_df['category'] = 'Undefined'\n",
    "    mld_2_df.loc[condition_better, 'category'] = 'Better'\n",
    "    mld_2_df.loc[condition_good, 'category'] = 'Good'\n",
    "    mld_2_df.loc[condition_bad, 'category'] = 'Bad'\n",
    "\n",
    "    # Create an empty DataFrame with columns\n",
    "    hourly_counts = pd.DataFrame(columns=['Bad', 'Better', 'Good', 'time'])\n",
    "\n",
    "    # Group by hour and category, then count occurrences\n",
    "    hourly_counts = mld_2_df.groupby([mld_2_df['date'].dt.floor('H'), 'category']).size().unstack(fill_value=0)\n",
    "\n",
    "    complete_hourly_index = pd.date_range(temp_df['date'].min(), temp_df['date'].max(), freq='H')\n",
    "    reference_df = pd.DataFrame(complete_hourly_index, columns=['date'])\n",
    "\n",
    "    # Add columns for 'Good', 'Better', and 'Bad' with zeros\n",
    "    reference_df['Good'] = 0\n",
    "    reference_df['Better'] = 0\n",
    "    reference_df['Bad'] = 0\n",
    "\n",
    "    missing_categories = set(['Bad', 'Better', 'Good']) - set(hourly_counts.columns)\n",
    "    for category in missing_categories:\n",
    "            hourly_counts[category] = 0\n",
    "\n",
    "    hourly_counts = pd.merge(reference_df, hourly_counts, how='left', on=['date'])\n",
    "    hourly_counts = hourly_counts[['date', 'Good_y', 'Better_y', 'Bad_y']]\n",
    "    hourly_counts = hourly_counts.rename(columns={'Good_y': 'Good', 'Better_y': 'Better', 'Bad_y':'Bad'})\n",
    "\n",
    "    hourly_counts = hourly_counts.fillna(0)\n",
    "    return hourly_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1701369000000  # define start time epoch\n",
    "end = 1703961000000   # end time epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_counts_mld_2 = mld_2_anode_status(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5458.0\n"
     ]
    }
   ],
   "source": [
    "total_anode_mld2 = hourly_counts_mld_2['Bad'].sum() + hourly_counts_mld_2['Better'].sum() + hourly_counts_mld_2['Good'].sum()\n",
    "print(total_anode_mld2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mld_1_anode_status(start, end):\n",
    "        tags= [\n",
    "                'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric',\n",
    "                'GAP_GAP03.PLC03.SCHENCK2_FEED_RATE',\n",
    "                'GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'\n",
    "            ]\n",
    "\n",
    "        temp_df = pd.DataFrame(columns=['time'])\n",
    "        temp_df.loc[0, 'time'] = start  # Insert 42 into 'Good' column at index 0\n",
    "        temp_df.loc[1, 'time'] = end\n",
    "        temp_df['time'] += (5 * 60 + 30) * 60 * 1000\n",
    "        temp_df['date'] = pd.to_datetime(temp_df['time'], unit='ms')\n",
    "\n",
    "        mld_1_df=getValues(tags, start, end)\n",
    "\n",
    "        mld_1_df = mld_1_df.dropna()\n",
    "        mld_1_df = mld_1_df[(mld_1_df['GAP_GAP03.PLC03.SCHENCK2_FEED_RATE'] >= 4000) & (mld_1_df['GAP_GAP03.PLC03.SCHENCK2_FEED_RATE'] < 6700) \n",
    "            & (mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] >= 1.56) & (mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] <= 1.69)]\n",
    "\n",
    "        mld_1_df = mld_1_df[mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] % 1 == 0]\n",
    "        mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] = mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'].astype(int)\n",
    "\n",
    "        # Find consecutive duplicate values in the column and mark them for removal\n",
    "        mld_1_df['to_remove'] = mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'] == mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Number'].shift(1)\n",
    "\n",
    "        mld_1_df = mld_1_df[mld_1_df['to_remove'] != True]\n",
    "\n",
    "        mld_1_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "        mld_1_df['time'] += (5 * 60 + 30) * 60 * 1000\n",
    "        mld_1_df['date'] = pd.to_datetime(mld_1_df['time'], unit='ms')\n",
    "\n",
    "        # Define conditions for categorizing 'GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'\n",
    "        condition_better = mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] >= 1.66\n",
    "        condition_good = (mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] >= 1.65) & (mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] < 1.66)\n",
    "        condition_bad = mld_1_df['GAP_GAP04.PLC04.MLD1_DATA_Anode_Geometric'] < 1.65\n",
    "\n",
    "        # Create a new column 'category' based on conditions\n",
    "        mld_1_df['category'] = 'Undefined'\n",
    "        mld_1_df.loc[condition_better, 'category'] = 'Better'\n",
    "        mld_1_df.loc[condition_good, 'category'] = 'Good'\n",
    "        mld_1_df.loc[condition_bad, 'category'] = 'Bad'\n",
    "\n",
    "        # Create an empty DataFrame with columns\n",
    "        hourly_counts = pd.DataFrame(columns=['Bad', 'Better', 'Good', 'time'])\n",
    "\n",
    "        # Group by hour and category, then count occurrences\n",
    "        hourly_counts = mld_1_df.groupby([mld_1_df['date'].dt.floor('H'), 'category']).size().unstack(fill_value=0)\n",
    "\n",
    "        complete_hourly_index = pd.date_range(temp_df['date'].min(), temp_df['date'].max(), freq='H')\n",
    "        reference_df = pd.DataFrame(complete_hourly_index, columns=['date'])\n",
    "\n",
    "        # Add columns for 'Good', 'Better', and 'Bad' with zeros\n",
    "        reference_df['Good'] = 0\n",
    "        reference_df['Better'] = 0\n",
    "        reference_df['Bad'] = 0\n",
    "\n",
    "        missing_categories = set(['Bad', 'Better', 'Good']) - set(hourly_counts.columns)\n",
    "        for category in missing_categories:\n",
    "                hourly_counts[category] = 0\n",
    "\n",
    "        hourly_counts = pd.merge(reference_df, hourly_counts, how='left', on=['date'])\n",
    "        hourly_counts = hourly_counts[['date', 'Good_y', 'Better_y', 'Bad_y']]\n",
    "        hourly_counts = hourly_counts.rename(columns={'Good_y': 'Good', 'Better_y': 'Better', 'Bad_y':'Bad'})\n",
    "\n",
    "        hourly_counts = hourly_counts.fillna(0)\n",
    "        return hourly_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_counts_mld_1 = mld_1_anode_status(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5466.0\n"
     ]
    }
   ],
   "source": [
    "total_anode_mld1 = hourly_counts_mld_1['Bad'].sum() + hourly_counts_mld_1['Better'].sum() + hourly_counts_mld_1['Good'].sum()\n",
    "print(total_anode_mld1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10924.0\n"
     ]
    }
   ],
   "source": [
    "total_anode = total_anode_mld1 + total_anode_mld2\n",
    "print(total_anode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
